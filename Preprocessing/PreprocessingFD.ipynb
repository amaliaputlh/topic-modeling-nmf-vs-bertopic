{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f20d09-5250-4a20-8cfd-e79769486d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dan install \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b53c01e-a3fb-49f5-b647-ffb21e3be01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Sastrawi in e:\\anaconda\\lib\\site-packages (1.0.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install Sastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d49f4279-0f29-4004-9821-7ad20c22bcba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pengguna</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Ulasan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nadanabilah_</td>\n",
       "      <td>5</td>\n",
       "      <td>Toner ini cocok untuk kulitku yang bruntusan. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>maidaaa</td>\n",
       "      <td>5</td>\n",
       "      <td>Ini nyaman di kulit untuk anti aging, aku yg s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>citraardini28</td>\n",
       "      <td>5</td>\n",
       "      <td>Aku udah coba Avoskin Miraculous Refining Tone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nahel</td>\n",
       "      <td>5</td>\n",
       "      <td>Teksturnya kental agak bau aneh tapi hasilnya ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ajeng1717</td>\n",
       "      <td>5</td>\n",
       "      <td>1. Suka banget sama tekstur tonernya yang ring...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Pengguna  Rating                                             Ulasan\n",
       "0   nadanabilah_       5  Toner ini cocok untuk kulitku yang bruntusan. ...\n",
       "1        maidaaa       5  Ini nyaman di kulit untuk anti aging, aku yg s...\n",
       "2  citraardini28       5  Aku udah coba Avoskin Miraculous Refining Tone...\n",
       "3          nahel       5  Teksturnya kental agak bau aneh tapi hasilnya ...\n",
       "4      ajeng1717       5  1. Suka banget sama tekstur tonernya yang ring..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LIHAT DATA\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"ulasan_avoskin_threading.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaa10939-4073-46e6-b426-ca48d85fb478",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LIHAT DUPLIKASI\n",
    "df.duplicated(subset=['Ulasan']).sum()\n",
    "df[df.duplicated(subset=['Ulasan'], keep=False)]\n",
    "df.drop_duplicates(subset=['Ulasan'], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69fff797-5d4f-4fa0-8e1d-f3bf90a2b247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Pengguna  Rating                                             Ulasan\n",
      "0   nadanabilah_       5  Toner ini cocok untuk kulitku yang bruntusan S...\n",
      "1        maidaaa       5  Ini nyaman di kulit untuk anti aging aku yg se...\n",
      "2  citraardini28       5  Aku udah coba Avoskin Miraculous Refining Tone...\n",
      "3          nahel       5  Teksturnya kental agak bau aneh tapi hasilnya ...\n",
      "4      ajeng1717       5  Suka banget sama tekstur tonernya yang ringan ...\n"
     ]
    }
   ],
   "source": [
    "#DATA CLEANING\n",
    "import re\n",
    "\n",
    "# Fungsi untuk membersihkan teks\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\d+', '', text)  # Hapus angka    \n",
    "    text = re.sub(r'@\\w+', '', text)  # Hapus mention\n",
    "    text = re.sub(r'#\\w+', '', text)  # Hapus hashtag\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Hapus karakter khusus (hanya menyisakan huruf dan spasi)\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)  # Menghapus huruf berulang lebih dari 2 kali\n",
    "    text = text.strip()  # Hapus spasi di awal & akhir\n",
    "    text = ' '.join([word for word in text.split() if len(word) > 1])  # Hapus kata satu huruf\n",
    "    return text\n",
    "\n",
    "# Terapkan pembersihan hanya pada kolom \"Ulasan\"\n",
    "df['Ulasan'] = df['Ulasan'].astype(str).apply(clean_text)\n",
    "\n",
    "# Simpan hasil ke file baru\n",
    "df.to_csv(\"ulasan_cleaned.csv\", index=False)\n",
    "\n",
    "# Tampilkan 5 data pertama setelah pembersihan\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b759002e-ee22-47b8-b8d2-e6d29130fd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Pengguna  Rating                                             Ulasan\n",
      "0   nadanabilah_       5  toner ini cocok untuk kulitku yang bruntusan s...\n",
      "1        maidaaa       5  ini nyaman di kulit untuk anti aging aku yg se...\n",
      "2  citraardini28       5  aku udah coba avoskin miraculous refining tone...\n",
      "3          nahel       5  teksturnya kental agak bau aneh tapi hasilnya ...\n",
      "4      ajeng1717       5  suka banget sama tekstur tonernya yang ringan ...\n"
     ]
    }
   ],
   "source": [
    "#CASE FOLDING\n",
    "# Load data hasil cleaning\n",
    "df = pd.read_csv(\"ulasan_cleaned.csv\")\n",
    "\n",
    "# Case Folding: Ubah semua teks menjadi huruf kecil\n",
    "df['Ulasan'] = df['Ulasan'].str.lower()\n",
    "\n",
    "# Simpan hasil case folding ke file CSV baru\n",
    "df.to_csv(\"ulasan_casefolded.csv\", index=False)\n",
    "\n",
    "# Cek hasilnya (opsional)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42b653df-2b9a-4cba-a09c-7800852958a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punkt ditemukan!\n",
      "                                              Ulasan  \\\n",
      "0  [toner, ini, cocok, untuk, kulitku, yang, brun...   \n",
      "1  [ini, nyaman, di, kulit, untuk, anti, aging, a...   \n",
      "2  [aku, udah, coba, avoskin, miraculous, refinin...   \n",
      "3  [teksturnya, kental, agak, bau, aneh, tapi, ha...   \n",
      "4  [suka, banget, sama, tekstur, tonernya, yang, ...   \n",
      "\n",
      "                                              Ulasan  \n",
      "0  [toner, ini, cocok, untuk, kulitku, yang, brun...  \n",
      "1  [ini, nyaman, di, kulit, untuk, anti, aging, a...  \n",
      "2  [aku, udah, coba, avoskin, miraculous, refinin...  \n",
      "3  [teksturnya, kental, agak, bau, aneh, tapi, ha...  \n",
      "4  [suka, banget, sama, tekstur, tonernya, yang, ...  \n",
      "        Pengguna  Rating                                             Ulasan\n",
      "0   nadanabilah_       5  [toner, ini, cocok, untuk, kulitku, yang, brun...\n",
      "1        maidaaa       5  [ini, nyaman, di, kulit, untuk, anti, aging, a...\n",
      "2  citraardini28       5  [aku, udah, coba, avoskin, miraculous, refinin...\n",
      "3          nahel       5  [teksturnya, kental, agak, bau, aneh, tapi, ha...\n",
      "4      ajeng1717       5  [suka, banget, sama, tekstur, tonernya, yang, ...\n"
     ]
    }
   ],
   "source": [
    "#TOKENIZATION\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print(\"Punkt ditemukan!\")\n",
    "except LookupError:\n",
    "    print(\"Punkt tidak ditemukan, perlu download ulang.\")\n",
    "    \n",
    "\n",
    "# Baca file CSV\n",
    "file_path = \"ulasan_casefolded.csv\" \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Lakukan tokenisasi pada kolom \"Ulasan\"\n",
    "df[\"Ulasan\"] = df[\"Ulasan\"].astype(str).apply(word_tokenize)\n",
    "\n",
    "# Tampilkan beberapa hasil pertama\n",
    "print(df[[\"Ulasan\", \"Ulasan\"]].head())\n",
    "\n",
    "# Simpan hasilnya ke CSV jika perlu\n",
    "df.to_csv(\"ulasan_tokenized.csv\", index=False)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a86d49f3-1552-454e-97c9-e18b49572407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kamus Typo Contoh (5): [('abnormal_words', 'words'), ('ngefeknya', 'ngefek'), ('bahagiabselalu', 'bahagia selalu'), ('moistutizer', 'moisturizer'), ('soso', 'medium')]\n",
      "âœ… Preprocessing selesai! Data tersimpan di 'ulasan_normalized_final.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "\n",
    "# Fungsi aman untuk mengonversi string list ke list Python\n",
    "def safe_eval(s):\n",
    "    try:\n",
    "        return ast.literal_eval(s)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return s.split()\n",
    "\n",
    "# Fungsi untuk membaca kamus dari file .txt dengan format kata_typo,perbaikan\n",
    "def load_kamus_typo(filename):\n",
    "    kamus = {}\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                parts = line.split(\",\")\n",
    "                if len(parts) == 2:\n",
    "                    key, value = map(str.strip, parts)\n",
    "                    kamus[key] = value\n",
    "    return kamus\n",
    "\n",
    "# Load kamus typo dari file\n",
    "kamus_typo = load_kamus_typo(\"kamus_typo.txt\")\n",
    "print(\"Kamus Typo Contoh (5):\", list(kamus_typo.items())[:5])\n",
    "\n",
    "# Normalisasi kata berdasarkan kamus typo\n",
    "def normalisasi_kamus(tokens, kamus):\n",
    "    return [kamus.get(token, token) for token in tokens]\n",
    "\n",
    "\n",
    "def normalize_hyphenated_words(tokens, kamus):\n",
    "    normalized_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if \"-\" in token:  # Cek apakah token mengandung tanda \"-\"\n",
    "            if token in kamus:  # Cek apakah ada di kamus secara langsung\n",
    "                normalized_tokens.append(kamus[token])  # Gunakan kata dari kamus\n",
    "            else:\n",
    "                # Pisahkan berdasarkan \"-\" lalu normalisasi bagian-bagiannya\n",
    "                parts = token.split(\"-\")\n",
    "                normalized_parts = [kamus.get(part, part) for part in parts]\n",
    "                normalized_tokens.append(\"-\".join(normalized_parts))  # Gabungkan kembali\n",
    "        else:\n",
    "            normalized_tokens.append(kamus.get(token, token))  # Normalisasi biasa\n",
    "            \n",
    "    return normalized_tokens\n",
    "\n",
    "\n",
    "# Normalisasi pengulangan huruf di akhir kata\n",
    "def normalize_trailing_repeated_letters(word):\n",
    "    return re.sub(r\"(.)\\1+$\", r\"\\1\", word)\n",
    "\n",
    "# Normalisasi pengulangan kata berturut-turut3\n",
    "def normalize_repeated_words(tokens):\n",
    "    return [tokens[i] if i == 0 or tokens[i] != tokens[i - 1] else f\"{tokens[i]}-{tokens[i]}\" for i in range(len(tokens))]\n",
    "\n",
    "# Hapus token yang terdiri dari 2 huruf, kecuali yang bermakna\n",
    "KATA_PENTING_2_HURUF = {\"di\", \"ke\", \"ya\", \"ku\", \"mu\", \"si\"}\n",
    "\n",
    "def hapus_token_pendek(tokens):\n",
    "    return [token for token in tokens if len(token) > 2 or token in KATA_PENTING_2_HURUF]\n",
    "\n",
    "#  **Pipeline Normalisasi**\n",
    "def normalisasi_pipeline(ulasan):\n",
    "    tokens = safe_eval(ulasan)\n",
    "    tokens = [normalize_trailing_repeated_letters(token) for token in tokens]\n",
    "    tokens = normalisasi_kamus(tokens, kamus_typo)\n",
    "    tokens = normalize_hyphenated_words(tokens, kamus_typo)\n",
    "    tokens = normalize_repeated_words(tokens)\n",
    "    tokens = normalisasi_kamus(tokens, kamus_typo)  # Pastikan semua kata sudah dinormalisasi\n",
    "    tokens = hapus_token_pendek(tokens)\n",
    "    return tokens\n",
    "\n",
    "# ðŸ—‚ **Load Data dan Terapkan Normalisasi**\n",
    "df = pd.read_csv(\"ulasan_tokenized.csv\")\n",
    "df[\"Ulasan\"] = df[\"Ulasan\"].apply(normalisasi_pipeline)\n",
    "df.to_csv(\"ulasan_normalized_final.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Preprocessing selesai! Data tersimpan di 'ulasan_normalized_final.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "be2ea1b0-18ab-40cf-bb05-b51658c614ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Normalisasi pengulangan suku kata selesai! Data disimpan di 'ulasan_normalized_final_v2.csv'\n"
     ]
    }
   ],
   "source": [
    "#NORMALISASI 3 (minim bug)\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#Fungsi untuk Memisahkan Kata Berulang (contoh: \"poripori\" â†’ \"pori-pori\")\n",
    "def split_repeated_syllables(word):\n",
    "    \"\"\"Mendeteksi pengulangan suku kata dan menambahkan tanda hubung (-).\"\"\"\n",
    "    pattern = r'(\\w{3,})\\1'  # Cari pola pengulangan minimal 2 huruf\n",
    "    return re.sub(pattern, r'\\1-\\1', word)\n",
    "\n",
    "#Terapkan Normalisasi pada Data\n",
    "df = pd.read_csv(\"ulasan_normalized_final.csv\")\n",
    "\n",
    "# Terapkan normalisasi suku kata berulang pada setiap token\n",
    "df[\"Ulasan\"] = df[\"Ulasan\"].apply(lambda tokens: [split_repeated_syllables(token) for token in eval(tokens)])\n",
    "\n",
    "#Simpan Hasil ke File Baru\n",
    "df.to_csv(\"ulasan_normalized_final_v2.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Normalisasi pengulangan suku kata selesai! Data disimpan di 'ulasan_normalized_final_v2.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8f32c1cd-556e-47fb-b4d9-1536e818c25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ulasan_no_stopwords.csv tersimpan\n",
      "        Pengguna  Rating                                             Ulasan\n",
      "0   nadanabilah_       5  [toner, cocok, kulitku, bruntusan, mengangkat,...\n",
      "1        maidaaa       5  [nyaman, kulit, anti, aging, sensitive, skin, ...\n",
      "2  citraardini28       5  [avoskin, miraculous, refining, toner, puas, h...\n",
      "3          nahel       5  [teksturnya, kental, bau, aneh, hasilnya, menc...\n",
      "4      ajeng1717       5  [suka, tekstur, tonernya, ringan, cepat, menye...\n"
     ]
    }
   ],
   "source": [
    "#STOPWORDS\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Stopwords bahasa Indonesia & Inggris\n",
    "stopwords_indonesia = set(stopwords.words('indonesian'))\n",
    "stopwords_inggris = set(stopwords.words('english'))\n",
    "\n",
    "# Stopwords tambahan (kustom)\n",
    "try:\n",
    "    with open(\"stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        stopwords_kustom = set(f.read().splitlines())  # Simpan dalam set\n",
    "except FileNotFoundError:\n",
    "    print(\"File stopwords.txt tidak ditemukan!\")\n",
    "    stopwords_kustom = set()\n",
    "\n",
    "# Gabungkan semua stopwords\n",
    "all_stopwords = stopwords_indonesia | stopwords_inggris | stopwords_kustom\n",
    "\n",
    "# Baca data yang telah ternormalisasi\n",
    "df = pd.read_csv(\"ulasan_normalized_final_v2.csv\")\n",
    "\n",
    "# Fungsi untuk menghapus stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in all_stopwords]\n",
    "\n",
    "# Terapkan stopwords removal pada kolom \"Ulasan\"\n",
    "df[\"Ulasan\"] = df[\"Ulasan\"].apply(lambda x: remove_stopwords(eval(x)))\n",
    "\n",
    "# Simpan hasil ke file baru\n",
    "df.to_csv(\"ulasan_no_stopwords.csv\", index=False)\n",
    "print(\"ulasan_no_stopwords.csv tersimpan\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2f9a8668-bee9-4c56-8e34-f9bdd79e9cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kata unik berhasil diekstrak! Total kata unik: 4869\n",
      "Cek selesai! Kata dikenal: 4184, Kata tidak dikenal: 685\n",
      "Kata dikenal disimpan di 'kata_dikenal.csv'\n",
      "Kata tidak dikenal disimpan di 'kata_tidak_dikenal.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load dataset ulasan\n",
    "df = pd.read_csv(\"ulasan_no_stopwords.csv\")\n",
    "\n",
    "# Fungsi untuk tokenisasi dan ekstraksi kata unik dari setiap ulasan\n",
    "def get_unique_words(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', str(text).lower())  # Ambil kata per kata (huruf dan angka)\n",
    "    return set(words)  # Ubah ke set agar hanya menyimpan kata unik\n",
    "\n",
    "# Gabungkan semua kata unik dalam dataset\n",
    "unique_words = set()\n",
    "df[\"Ulasan\"].dropna().apply(lambda x: unique_words.update(get_unique_words(x)))\n",
    "\n",
    "# Simpan kata unik ke DataFrame\n",
    "df_unique = pd.DataFrame(list(unique_words), columns=[\"Kata\"])\n",
    "\n",
    "# Simpan kata unik ke CSV\n",
    "df_unique.to_csv(\"kata_unik.csv\", index=False)\n",
    "print(f\"Kata unik berhasil diekstrak! Total kata unik: {len(df_unique)}\")\n",
    "\n",
    "# Load daftar kata dasar dari vocab.txt\n",
    "with open(\"vocab.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    kata_baku = set(f.read().splitlines())  # Simpan dalam set untuk pencarian cepat\n",
    "\n",
    "# Fungsi untuk mengecek dan mengubah kata dengan imbuhan menjadi kata dasar\n",
    "def normalize_word(word, vocab):\n",
    "    if word in vocab:\n",
    "        return word\n",
    "    \n",
    "    # ðŸ”¹ Daftar imbuhan awalan dan akhiran\n",
    "    prefixes = [\"ku\",\"di\", \"me\", \"men\", \"meng\", \"ber\", \"ter\", \"nge\", \"se\"]\n",
    "    suffixes = [\"nya\", \"ku\", \"kan\", \"in\", \"mu\"]\n",
    "    \n",
    "    # ðŸ”¹ Tangani \"nge-in\" â†’ kata dasar\n",
    "    if word.startswith(\"nge\") and word.endswith(\"in\"):\n",
    "        base = word[3:-2]\n",
    "        if base in vocab:\n",
    "            return base\n",
    "    \n",
    "    # ðŸ”¹ Tangani \"di-kan\", tetapi tetap dianggap kata dikenal jika kata dasarnya ada di vocab\n",
    "    if word.startswith(\"di\") and word.endswith(\"kan\"):\n",
    "        base = word[2:-3]  # Hapus \"di\" dan \"kan\"\n",
    "        if base in vocab:\n",
    "            return word  # Tidak mengubah ke kata dasar, tetap di daftar kata dikenal\n",
    "    \n",
    "    # ðŸ”¹ Cek kata dengan awalan\n",
    "    for prefix in prefixes:\n",
    "        if word.startswith(prefix):\n",
    "            base = word[len(prefix):]\n",
    "            if base in vocab:\n",
    "                return base\n",
    "    \n",
    "    # ðŸ”¹ Cek kata dengan akhiran\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            base = word[:-len(suffix)]\n",
    "            if base in vocab:\n",
    "                return base\n",
    "    \n",
    "    return word\n",
    "\n",
    "# Normalisasi kata berdasarkan vocab\n",
    "df_unique[\"Kata Normalized\"] = df_unique[\"Kata\"].apply(lambda x: normalize_word(x, kata_baku))\n",
    "\n",
    "# Simpan kata yang tidak dikenal untuk analisis lebih lanjut\n",
    "kata_dikenal = df_unique[(df_unique[\"Kata Normalized\"].isin(kata_baku)) | (df_unique[\"Kata\"].str.startswith(\"di\") & df_unique[\"Kata\"].str.endswith(\"kan\"))]\n",
    "kata_tidak_dikenal = df_unique[~df_unique.index.isin(kata_dikenal.index)]\n",
    "\n",
    "kata_dikenal.to_csv(\"kata_dikenal.csv\", index=False)\n",
    "kata_tidak_dikenal.to_csv(\"kata_tidak_dikenal.csv\", index=False)\n",
    "\n",
    "print(f\"Cek selesai! Kata dikenal: {len(kata_dikenal)}, Kata tidak dikenal: {len(kata_tidak_dikenal)}\")\n",
    "print(\"Kata dikenal disimpan di 'kata_dikenal.csv'\")\n",
    "print(\"Kata tidak dikenal disimpan di 'kata_tidak_dikenal.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "321826e9-a9d5-4164-89b3-05c3a26c98fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peringatan: Baris tidak valid di kamus_typo.txt -> ''\n",
      "Peringatan: Baris tidak valid di kamus_typo.txt -> ''\n",
      "Kata yang seharusnya ternormalisasi tetapi masih tidak dikenali:\n",
      "{'apapun', 'dipakainya', 'nginkan', 'malem', 'temen', 'nunggu', 'phte', 'bgtbgt', 'guys', 'mid', 'temenku', 'barengan', 'hehe', 'bhap', 'nyesel', 'hua', 'udah', 'gantian', 'nyoba', 'bnr', 'so', 'clekit', 'bling', 'dilengkapi', 'grenjel', 'diambilnya', 'saidhada', 'breng', 'lindungi'}\n"
     ]
    }
   ],
   "source": [
    "kamus_typo = {}\n",
    "\n",
    "# Load kamus typo dengan pemisah koma\n",
    "with open(\"kamus_typo.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if \",\" in line:  # Pastikan ada pemisah \",\"\n",
    "            typo, correct = line.split(\",\", 1)  # Split hanya pada koma pertama\n",
    "            kamus_typo[typo.strip()] = correct.strip()\n",
    "        else:\n",
    "            print(f\"Peringatan: Baris tidak valid di kamus_typo.txt -> '{line}'\")\n",
    "\n",
    "# Load kata tidak dikenal dari CSV\n",
    "import pandas as pd\n",
    "\n",
    "df_kata_tidak_dikenal = pd.read_csv(\"kata_tidak_dikenal.csv\", header=None)  # Pastikan tidak ada header\n",
    "kata_tidak_dikenal = set(df_kata_tidak_dikenal.iloc[:, 0])  # Ambil kolom pertama\n",
    "\n",
    "# Cari kata yang seharusnya ternormalisasi tetapi masih tidak dikenali\n",
    "kata_masih_tidak_ternormalisasi = kata_tidak_dikenal.intersection(kamus_typo.keys())\n",
    "\n",
    "print(\"Kata yang seharusnya ternormalisasi tetapi masih tidak dikenali:\")\n",
    "print(kata_masih_tidak_ternormalisasi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a40ebec1-19dc-4a45-8215-ca3a7de26f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming selesai, hasil disimpan di ulasan_stemmed_xx.csv\n"
     ]
    }
   ],
   "source": [
    "#STEMMING\n",
    "import pandas as pd\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import ast\n",
    "\n",
    "# Buat Stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# Baca data setelah stopwords removal\n",
    "df = pd.read_csv(\"ulasan_no_stopwords.csv\")\n",
    "\n",
    "# Fungsi stemming\n",
    "def stemming_tokens(tokens):\n",
    "    return [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "# Terapkan stemming pada kolom \"Ulasan\"\n",
    "df[\"Ulasan\"] = df[\"Ulasan\"].apply(lambda x: stemming_tokens(ast.literal_eval(x)))\n",
    "\n",
    "# Simpan hasil ke file baru\n",
    "df.to_csv(\"ulasan_stemmed_avoskin.csv\", index=False)\n",
    "\n",
    "print(\"Stemming selesai, hasil disimpan di ulasan_stemmed_xx.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e18e41a6-3bf2-4c2e-94d4-ade184ac5229",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STOPWORDS\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Stopwords bahasa Indonesia & Inggris\n",
    "stopwords_indonesia = set(stopwords.words('indonesian'))\n",
    "stopwords_inggris = set(stopwords.words('english'))\n",
    "\n",
    "# Stopwords tambahan (kustom)\n",
    "try:\n",
    "    with open(\"stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        stopwords_kustom = set(f.read().splitlines())  # Simpan dalam set\n",
    "except FileNotFoundError:\n",
    "    print(\"File stopwords.txt tidak ditemukan!\")\n",
    "    stopwords_kustom = set()\n",
    "\n",
    "# Gabungkan semua stopwords\n",
    "all_stopwords = stopwords_indonesia | stopwords_inggris | stopwords_kustom\n",
    "\n",
    "# Baca data yang telah ternormalisasi\n",
    "df = pd.read_csv(\"ulasan_documents.csv\")\n",
    "\n",
    "# Fungsi untuk menghapus stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in all_stopwords]\n",
    "\n",
    "# Terapkan stopwords removal pada kolom \"Ulasan\"\n",
    "df[\"Ulasan\"] = df[\"Ulasan\"].apply(lambda x: remove_stopwords(eval(x)))\n",
    "\n",
    "# Simpan hasil ke file baru\n",
    "df.to_csv(\"ulasan_preprocessed_avoskin.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e988254-d852-41d3-885f-f665afce5670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Pengguna  Rating                                             Ulasan\n",
      "0       fonimeilisa       5  ['suka', 'eksfoliasi', 'toner', 'keren', 'prod...\n",
      "1         latifalya       4  ['bangga', 'habis', 'langsung', 'repurchase', ...\n",
      "2          vaniashr       5  ['barusan', 'coba', 'produk', 'kemarin', 'beli...\n",
      "3        luthfiahnn       5  ['bagus', 'banget', 'bikin', 'kulit', 'mulus',...\n",
      "4          faradits       5  ['jujur', 'bagus', 'tidak tahu', 'avoskin', 'm...\n",
      "...             ...     ...                                                ...\n",
      "1995     ndtsabilah       5  ['pakai', 'minggu', 'efek', 'pakai', 'perih', ...\n",
      "1996     Regyaviony       4  ['toner', 'kandung', 'aktif', 'pakai', 'rada',...\n",
      "1997       domokyun       3  ['tonernya', 'cair', 'aroma', 'lucu', 'asem-as...\n",
      "1998      fairuzs13       5  ['eksfoliasi', 'toner', 'semenjak', 'pandemi',...\n",
      "1999  ratihtresna15       4  ['produk', 'bagus', 'bikin', 'muka', 'lembab',...\n",
      "\n",
      "[2000 rows x 3 columns]\n",
      "           Pengguna  Rating                                             Ulasan\n",
      "0       fonimeilisa       5  suka eksfoliasi toner keren product local mid ...\n",
      "1         latifalya       4  bangga habis langsung repurchase mantap breako...\n",
      "2          vaniashr       5  barusan coba produk kemarin beli event surabay...\n",
      "3        luthfiahnn       5  bagus banget bikin kulit mulus komedo hitam pu...\n",
      "4          faradits       5  jujur bagus tidak tahu avoskin miraculous refi...\n",
      "...             ...     ...                                                ...\n",
      "1995     ndtsabilah       5  pakai minggu efek pakai perih perih bagi masal...\n",
      "1996     Regyaviony       4  toner kandung aktif pakai rada perih perih muk...\n",
      "1997       domokyun       3  tonernya cair aroma lucu asemasem toner rendah...\n",
      "1998      fairuzs13       5  best eksfoliasi toner semenjak pandemi kerja p...\n",
      "1999  ratihtresna15       4  produk bagus bikin muka lembab cerah sayang mu...\n",
      "\n",
      "[2000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(repr(df_stemmed))\n",
    "print(repr(df_preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee899c6f-d30b-4826-9e41-ede39ac7911b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming multibahasa selesai, hasil disimpan di ulasan_stemmed_multilang.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Inisialisasi stemmer Indonesia dan Inggris\n",
    "factory = StemmerFactory()\n",
    "ind_stemmer = factory.create_stemmer()\n",
    "eng_stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Baca file hasil stopwords removal\n",
    "df = pd.read_csv(\"ulasan_no_stopwords.csv\")\n",
    "\n",
    "# Fungsi untuk stem token berdasarkan bahasanya\n",
    "def stemming_multilang(tokens):\n",
    "    stemmed_tokens = []\n",
    "    for word in tokens:\n",
    "        # Jika karakter semua ASCII dan tidak ada karakter aneh â†’ diasumsikan Bahasa Inggris\n",
    "        if word.isalpha() and all(ord(c) < 128 for c in word):\n",
    "            stemmed_word = eng_stemmer.stem(word)\n",
    "        else:\n",
    "            stemmed_word = ind_stemmer.stem(word)\n",
    "        stemmed_tokens.append(stemmed_word)\n",
    "    return stemmed_tokens\n",
    "\n",
    "# Terapkan stemming multibahasa\n",
    "df[\"Ulasan\"] = df[\"Ulasan\"].apply(lambda x: stemming_multilang(ast.literal_eval(x)))\n",
    "\n",
    "# Simpan hasilnya\n",
    "df.to_csv(\"ulasan_stemmed_multilang.csv\", index=False)\n",
    "print(\"Stemming multibahasa selesai, hasil disimpan di ulasan_stemmed_multilang.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8fca68-56c5-40a0-b35c-3bc78f2b396e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
